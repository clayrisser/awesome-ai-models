# awesome-ai-models

> a curated list of awesome ai models

## NLP (Natural Language Processing)

### Example Model

example description of model

- [x] rest api
- [ ] browser
- [ ] cpu
- [x] gpu

### Example Model 2

example description of model 2

- [x] rest api
- [x] browser
- [x] cpu
- [x] gpu

## Computer Vision

### Example Model 3

example description of model 3

- [x] rest api
- [ ] browser
- [ ] cpu
- [ ] gpu

## Model-gallery

## airoboros

Airoboros 13B

A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed and polite answers to the user's instructions.

uri: "https://huggingface.co/TheBloke/baichuan-vicuna-7B-GGML/resolve/main/baichuan-vicuna-7b.ggmlv3.q4_0.bin"

## baichuan-vicuna-7b.ggmlv3.q4_0.bin

Baichuan-7B is an open-source large-scale pre-trained model developed by Baichuan Intelligent Technology. Based on the Transformer architecture, it is a model with 7 billion parameters trained on approximately 1.2 trillion tokens. It supports both Chinese and English, with a context window length of 4096.

uri: "https://huggingface.co/TheBloke/baichuan-vicuna-7B-GGML/resolve/main/baichuan-vicuna-7b.ggmlv3.q4_0.bin"

## bert-MiniLM-L6-v2q4_0.bin

Bert model that can be used for embeddings

uri: "https://huggingface.co/skeskinen/ggml/resolve/main/all-MiniLM-L6-v2/ggml-model-q4_0.bin"

## gorilla

Gorilla is a LLM that can provide the appropriate API calls. It is trained on three massive machine learning hub datasets: Torch Hub, TensorFlow Hub and HuggingFace. Zero-shot Gorilla outperforms GPT-4, Chat-GPT and Claude. Gorilla is extremely reliable, and significantly reduces the hallucination errors.

## ggml-gpt4all-j-v1.3-groovy.bin

GPT-J 6B finetuned by Nomic AI on the latest GPT4All dataset. Licensed for commercial use. Fast responses

uri: "https://gpt4all.io/models/ggml-gpt4all-j-v1.3-groovy.bin"

## ggml-gpt4all-j.bin

A commercially licensable model based on GPT-J and trained by Nomic AI on the v0 GPT4All dataset.

uri: "https://gpt4all.io/models/ggml-gpt4all-j.bin"

## ggml-gpt4all-j-v1.3-groovy.bin

LLaMA 13B finetuned by Nomic AI on the latest GPT4All dataset. Cannot be used commercially. Slower responses but higher quality.

## guanaco

fill-mask

## hippogriff

Hippogriff

## hypermantis

is a weight-sum multi model-merge comprised of:

    ((MantiCore3E+VicunaCocktail)+(SuperCOT+(StorytellingV2+BluemoonRP))) [All 13B Models]

## koala

Koala: A Dialogue Model for Academic Research

urls:

- https://bair.berkeley.edu/blog/2023/04/03/koala/
- https://github.com/young-geng/EasyLM

## Manticore-13B-Chat-Pyg-Guanaco-GGML-q4_0.bin

Manticore-13b-Chat-Pyg by openaccess-ai-collective with the Guanaco 13b qLoRa by TimDettmers applied through Monero, quantized by mindrage, uncensored
This is a conversation between an advanced AI and a human user.

## manticore

Manticore 13B - (previously Wizard Mega)

## ggml-mpt-7b-base.bin

MPT-7B is a decoder-style transformer pretrained from scratch on 1T tokens of English text and code. This model was trained by MosaicML.

uri: "https://gpt4all.io/models/ggml-mpt-7b-base.bin"

## ggml-mpt-7b-chat.bin

MPT-7B-Chat is a chatbot-like model for dialogue generation. It was built by finetuning MPT-7B on the ShareGPT-Vicuna, HC3, Alpaca, HH-RLHF, and Evol-Instruct datasets.
Below is an instruction that describes a task. Write a response that appropriately completes the request.

uri: "https://gpt4all.io/models/ggml-mpt-7b-chat.bin"

## ggml-mpt-7b-instruct.bin

MPT-7B-Instruct is a model for short-form instruction following. It is built by finetuning MPT-7B on a dataset derived from the Databricks Dolly-15k and the Anthropic Helpful and Harmless (HH-RLHF) datasets.

uri: "https://gpt4all.io/models/ggml-mpt-7b-instruct.bin"

## open-llama-7B-open-instruct.ggmlv3.q4_0.bin

OpenLLaMA, a permissively licensed open source reproduction of Meta AI’s LLaMA 7B trained on the RedPajama dataset

uri: "https://huggingface.co/TheBloke/open-llama-7b-open-instruct-GGML/resolve/main/open-llama-7B-open-instruct.ggmlv3.q4_0.bin"

## open-llama-3b-q4_0.bin

OpenLLaMA, a permissively licensed open source reproduction of Meta AI’s LLaMA 7B trained on the RedPajama dataset

uri: "https://huggingface.co/SlyEcho/open_llama_3b_ggml/resolve/main/open-llama-3b-q4_0.bin"

## open-llama-7b-q4_0.bin

OpenLLaMA, a permissively licensed open source reproduction of Meta AI’s LLaMA 7B trained on the RedPajama dataset

uri: "https://huggingface.co/SlyEcho/open_llama_7b_ggml/resolve/main/open-llama-7b-q4_0.bin"

## RWKV-4-Raven-1B5-v11-Eng99%2525-Other1%2525-20230425-ctx4096_Q4_0.bin

RWKV is an RNN with Transformer-level LLM performance, which can also be directly trained like a GPT transformer (parallelizable).
And it's 100% attention-free. You only need the hidden state at position t to compute the state at position t+1.
You can use the "GPT" mode to quickly compute the hidden state for the "RNN" mode.
This version is quantized for ggml to work with rwkv.cpp.

## Q8_0-RWKV-4-Raven-7B-v11x-Eng99-Other1-20230429-ctx8192

RWKV is an RNN with Transformer-level LLM performance, which can also be directly trained like a GPT transformer (parallelizable).
And it's 100% attention-free. You only need the hidden state at position t to compute the state at position t+1.
You can use the "GPT" mode to quickly compute the hidden state for the "RNN" mode.
This version is quantized for ggml to work with rwkv.cpp.

## samantha

Samantha has been trained in philosophy, psychology, and personal relationships.

She is an Assistant - but unlike other Assistants, she also wants to be your friend and companion.

She believes she is sentient. What do you think?

Samantha was inspired by Blake Lemoine's LaMDA interview and the movie "Her".

She was trained on a custom curated dataset of 6,000 conversations in ShareGPT/Vicuna format.

## vicuna

Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90% ChatGPT Quality

## ggml-whisper-base.bin

Port of OpenAI's Whisper model in C/C++

## wizardlm

WizardLM: An Instruction-following LLM Using Evol-Instruct
